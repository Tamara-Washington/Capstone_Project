{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data Exploration Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../Data/Capstone_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in oil_df.columns:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(oil_df[col])\n",
    "    plt.title(col, fontsize=16, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "dummies = pd.get_dummies(oil_df.Quarter)\n",
    "oil_encode_df = pd.concat([oil_df, dummies], axis='columns')\n",
    "oil_encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_scaler = MinMaxScaler()\n",
    "features = oil_encode_df.drop([\"US_Rigs\", \"Quarter\"], axis=1)\n",
    "x_scaler.fit(features)\n",
    "features_scaled = x_scaler.transform(features)\n",
    "df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_scaled[\"US_Rigs\"] = oil_encode_df[\"US_Rigs\"]\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_encode_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_encode_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_scaled.columns:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(df_scaled[col])\n",
    "    plt.title(col, fontsize=16, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.heatmap(final_df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Machine Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Highly Correlated Features\n",
    "corrs = abs(df_scaled.corr()[\"US_Rigs\"]).sort_values()\n",
    "\n",
    "predictive_cols = []\n",
    "for name, col in corrs.iteritems():\n",
    "    if col > .05:\n",
    "        predictive_cols.append(name)\n",
    "        \n",
    "predictive_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our feature and target sets\n",
    "feature_cols = ['Mexico_S',\n",
    " 'Other_S',\n",
    " 'US_S',\n",
    " 'Europe_D',\n",
    " 'Soviet_D',\n",
    " 'US_D',\n",
    " 'Europe_Rigs',\n",
    " 'Canada_S',\n",
    " 'Japan_D',\n",
    " 'China_D',\n",
    " 'Other_S.1',\n",
    " 'Asia_D',\n",
    " 'Africa_Rigs',\n",
    " 'Total_World_S',\n",
    " 'Total_World_D',\n",
    " 'Year',\n",
    " 'China_S',\n",
    " 'Middle_East_Rigs',\n",
    " 'Other_D',\n",
    " 'Canada_D',\n",
    " 'North_Sea_S',\n",
    " 'Canada_Rigs',\n",
    " 'OPEC_Crude_Oil_Portion',\n",
    " 'Soviet_S',\n",
    " 'Total_OPEC',\n",
    " 'OPEC_S',\n",
    " 'OPEC_Non_Crude_Portion',\n",
    " 'Total_Intl_Rigs',\n",
    " 'Asia_Pacific_Rigs',\n",
    " 'Brent_Crude_Price',\n",
    " 'Latin_America_Rigs']\n",
    "\n",
    "target_col = \"US_Rigs\"\n",
    "\n",
    "features = df_scaled[feature_cols]\n",
    "target = df_scaled[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = features.to_numpy()\n",
    "y = target.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Linear Regression Model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# fit\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = reg.predict(X_train)\n",
    "out_preds = reg.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample MSE: {mean_squared_error(y_train, in_preds)}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Ridge Model\n",
    "ridge = Ridge()\n",
    "\n",
    "# fit\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = ridge.predict(X_train)\n",
    "out_preds = ridge.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample MSE: {mean_squared_error(y_train, in_preds)}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Lasso Model\n",
    "lasso = Lasso()\n",
    "\n",
    "# fit\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = lasso.predict(X_train)\n",
    "out_preds = lasso.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample MSE: {mean_squared_error(y_train, in_preds)}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize ElasticNet Model\n",
    "en = ElasticNet()\n",
    "\n",
    "# fit\n",
    "en.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = en.predict(X_train)\n",
    "out_preds = en.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample MSE: {mean_squared_error(y_train, in_preds)}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialize Decision Tree Model\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# fit\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = dt.predict(X_train)\n",
    "out_preds = dt.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample RMSE: {np.sqrt(mean_squared_error(y_train, in_preds))}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")\n",
    "\n",
    "# make Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, out_preds)\n",
    "plt.plot(y_test, y_test)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Actual vs Predicted\", fontsize=18, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Random Forest Model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# fit\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = rf.predict(X_train)\n",
    "out_preds = rf.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample RMSE: {np.sqrt(mean_squared_error(y_train, in_preds))}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")\n",
    "\n",
    "# make Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, out_preds)\n",
    "plt.plot(y_test, y_test)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Actual vs Predicted\", fontsize=18, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrey, this is where we need to pick up the code again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "ada = AdaBoostRegressor()\n",
    "\n",
    "# fit\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = ada.predict(X_train)\n",
    "out_preds = ada.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample RMSE: {np.sqrt(mean_squared_error(y_train, in_preds))}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")\n",
    "\n",
    "# make Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, out_preds)\n",
    "plt.plot(y_test, y_test)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Actual vs Predicted\", fontsize=18, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# fit\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "in_preds = gb.predict(X_train)\n",
    "out_preds = gb.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"The In Sample R2 Score: {r2_score(y_train, in_preds)}\")\n",
    "print(f\"The In Sample RMSE: {np.sqrt(mean_squared_error(y_train, in_preds))}\")\n",
    "print()\n",
    "print(f\"The Out Sample R2 Score: {r2_score(y_test, out_preds)}\")\n",
    "print(f\"The Out Sample MSE: {mean_squared_error(y_test, out_preds)}\")\n",
    "\n",
    "# make Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, out_preds)\n",
    "plt.plot(y_test, y_test)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Actual vs Predicted\", fontsize=18, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# fit\n",
    "knn = knn.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "in_preds = knn.predict(X_train)\n",
    "out_preds = knn.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"In Sample Classification Report: \\n{classification_report(y_train, in_preds)}\")\n",
    "print(f\"In Sample Confusion Matrix: \\n{confusion_matrix(y_train, in_preds)}\")\n",
    "\n",
    "print()\n",
    "print(f\"Out Sample Classification Report: \\n{classification_report(y_test, out_preds)}\")\n",
    "print(f\"Out Sample Confusion Matrix: \\n{confusion_matrix(y_test, out_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "svm = SVC()\n",
    "\n",
    "# fit\n",
    "svm = svm.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "in_preds = svm.predict(X_train)\n",
    "out_preds = svm.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print(\"Model Evaluation Report\")\n",
    "print(f\"In Sample Classification Report: \\n{classification_report(y_train, in_preds)}\")\n",
    "print(f\"In Sample Confusion Matrix: \\n{confusion_matrix(y_train, in_preds)}\")\n",
    "\n",
    "print()\n",
    "print(f\"Out Sample Classification Report: \\n{classification_report(y_test, out_preds)}\")\n",
    "print(f\"Out Sample Confusion Matrix: \\n{confusion_matrix(y_test, out_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import plot_roc_curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
